<p><i>This is the third post in the Attention Economy series. Here are the <a href="/#!post/attention-economy">first</a> and <a href="/#!post/apps-are-bad">second</a> posts.</i></p>
<hr>
<p>
  In 2006, the Harvard Business Review magazine published an article titled <a href="https://hbr.org/2006/01/evidence-based-management" target="_blank">"Evidence-Based Management."</a> It opens like this:
</p>
<p><i>
    A bold new way of thinking has taken the medical establishment by storm in the past decade: the idea that decisions in medical care should be based on the latest and best knowledge of what actually works. Dr. David Sackett, the individual most associated with evidence-based medicine, defines it as “the conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients.” Sackett, his colleagues at McMaster University in Ontario, Canada, and the growing number of physicians joining the movement are committed to identifying, disseminating, and, most importantly, applying research that is soundly conducted and clinically relevant.
</i></p>
<p>
  I don't think this will be a controversial stance among my readers. Science, i.e. the systematic search for knowledge and truth via testable hypotheses and reproducible experimentation, is pretty cool. Doctors should absolutely rely on science when making decisions about healthcare.
</p><p>
  In the two paragraphs after this, the authors argue that like doctors, managers should rely on science and evidence to make their decisions. They discuss various findings by researchers that go against standard practice in both medicine and organiztional management, but then they then do a very tricky, perhaps accidental sleight of hand; they conflate science done by scientific practitioners with "science" using data internally within a corporation. It starts with this anecdote:
</p>
<i><p>
    When it comes to setting the tone for evidence-based management, we have met few chief executives on a par with Kent Thiry, the CEO of DaVita, a $2 billion operator of kidney dialysis centers headquartered in El Segundo, California. Thiry joined DaVita in October 1999, when the company was in default on its bank loans, could barely meet payroll, and was close to bankruptcy. A big part of his turnaround effort has been to educate the many facility administrators, a large proportion of them nurses, in the use of data to guide their decisions.
  </p>
  <p>
    To ensure that the company has the information necessary to assess its operations, the senior management team and DaVita’s chief technical officer, Harlan Cleaver, have been relentless in building and installing systems that help leaders at all levels understand how well they are doing. One of Thiry’s mottoes is “No brag, just facts.” When he stands up at DaVita Academy, a meeting of about 400 frontline employees from throughout the organization, and states that the company has the best quality of treatment in the industry, that assertion is demonstrated with specific, quantitative comparisons.
  </p>
  <p>
    A large part of the company’s culture is a commitment to the quality of patient care. To reinforce this value, managers always begin reports and meetings with data on the effectiveness of the dialysis treatments and on patient health and well-being. And each facility administrator gets an eight-page report every month that shows a number of measures of the quality of care, which are summarized in a DaVita Quality Index. This emphasis on evidence also extends to management issues—administrators get information on operations, including treatments per day, teammate (employee) retention, the retention of higher-paying private pay patients, and a number of resource utilization measures such as labor hours per treatment and controllable expenses.
  </p>
</i>
<p>
  And now I have objections. Science is hard. So hard, in fact, that trained and dedicated scientists struggle with it. This is perhaps best exemplified by the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a>, which calls into question the credibility of much of our scentific knowledge. <!-- The scientific community tries to mitigate the innate difficulty of understanding the universe by developing community-wide standards and best practices, like <a href="https://en.wikipedia.org/wiki/Peer_review">peer review</a>. They usually organize themselves into universities that each employ hundreds if not thousands of professors overseeing and collaborating with various kinds of students and/or researchers to facilitate the complex, intergenerational process of learning necessary to do good science. -->
</p>
<p>
  Comparing corporations to the scientific community makes it obvious that these organizations do not exist to accomplish the same function. First, the obvious: corporations seek profit, while scientists seek truth. Profit, a corporation's true purpose, and its mission or stated societal role, are often in tension. The tension here is obvious in the anecdote provided above when the company's CTO "states that the company has the best quality of treatment in the industry, that assertion is demonstrated with specific, quantitative comparisons." Yet, in the proceeding paragraph, the examples given for these quantiative comparisons are "treatments per day, teammate (employee) retention, the retention of higher-paying private pay patients, and a number of resource utilization measures such as labor hours per treatment and controllable expenses."
</p>
<p>
  The claim that the company has the "highest quality treatment" is obviously intermingled with the company's profitability. That is bad science. The 400 nurses that Mr. Thiry "educated," had we their side of the story, would probably tell you that spending less time with patients will generally decrease the quality of care while increasing profit.
</p>
<p>
  I am not saying that corporations should avoid using metrics — I don't even think corporations as we know them should exist. I am saying that with this emphasis on being "data-driven" and "scientific," the business community has created a tradition much like those derided in this HBR article. They mime the aesthetic of science without its substance. Self-declared data-driven businesses are a <a href="https://en.wikipedia.org/wiki/Cargo_cult">cargo cult</a>, mimicking scientific ritual without understading how they actually work.
</p>
<h3>The Science Cargo Cult Made The Attention Economy</h3>
<p>
  I propose that the attention economy exists because measuring user engagement is the single easiest thing for tech companies to measure. When an organization decides to make rationality core to its identity, the organization will prioritize projects by whether or not they produce data. It was not necessarily most profitable for tech companies to focus on maximizing engagement in order to hoard user data; instead, these companies seek data to satisfy their cultural need to appear rational and scientific. By "being data-driven," they have chosen to focus their efforts on the most easily measurable things.
</p>
<p>
  I chose the 2006 HBR article to start because this desire for data-driven decision making in business predates the modern age of big tech. It is the cultural milieu into which they were born. When the nature of the internet made it so easy to generate data, the businesses' cultural desire for data fundamentally shaped how they made the entire industry.
</p>
<p>
  You can see these cultural values shaping the business in this much more recent article titled "<a href="https://hbr.org/2020/02/10-steps-to-creating-a-data-driven-culture">10 Steps to Creating a Data-Driven Culture</a>." Several of the 10 steps recomend changing the way products are made. Here is number 6, titled "make proofs of concept simple and robust, not fancy and brittle:"
</p>
<i>
  <p>
    In analytics, promising ideas greatly outnumber practical ones. Often, it’s not until firms try to put proofs of concept into production that the difference becomes clear. One large insurer held an internal hackathon and crowned its winner — an elegant improvement of an online process — only to scrap the idea because it seemed to require costly changes to underlying systems. Snuffing out good ideas in this way can be demoralizing for organizations.
  </p>
  <p>
    A better approach is to engineer proofs of concept where a core part of the concept is its viability in production. One good way is to start to build something that is industrial grade but trivially simple, and later ratchet up the level of sophistication. For example, to implement new risk models on a large, distributed computing system, a data products company started by implementing an extremely basic process that worked end-to-end: a small dataset flowed correctly from source systems and through a simple model and was then transmitted to end users. Once that was in place, and knowing that the whole still cohered, the firm could improve each component independently: greater data volumes, more exotic models, and better runtime performance.
  </p>
</i>
<p>
  The story of the data products company shows us it is better to make a simple product that gets the data flowing, juxtaposed with the less desirable user experience improvements that are too expensive to implement. Again, I want to reiterate that I do not really care whether this is good business advice or not. Instead, I wish to point out that taken in aggregate, advice like this has created the technological world we know. Companies prioritize simple products with measurable results vs complex undertakings that they freely admit would improve user experience. They are explicitly optimizing their measurability/effort ratio; they seek to decrease the amount of work they put into a project before they can start applying metrics to it.
</p>


<!-- https://hbr.org/2006/01/evidence-based-management -->
<!-- https://www.tableau.com/learn/articles/how-to-build-a-data-driven-organization -->
<!-- https://www.tableau.com/why-tableau/data-culture -->
<!-- https://hbr.org/2020/02/10-steps-to-creating-a-data-driven-culture -->
<!-- https://hbr.org/2022/02/why-becoming-a-data-driven-organization-is-so-hard -->
